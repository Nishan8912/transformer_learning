{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85402ece",
   "metadata": {},
   "source": [
    "## Transformer architecture from attention is all you need paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c230dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step1:Positional Encoding implementation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # create a matrix of shape (max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # shape (0, max_len) ----> shape (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # captures the positional relationships \n",
    "\n",
    "        # fill the positional encoding matrix with sine for even  and consine for odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) \n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # add batch dimension for broadcasting\n",
    "        pe = pe.unsqueeze(0) # shape (1, max_len, d_model)\n",
    "        \n",
    "        # Register as buffer so it's saved with the model but not considered a parameter\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape (batch_size, seq_len, d_model)\n",
    "        # get the positional encoding for the input sequence length\n",
    "        return x + self.pe[:, :x.size(1), :].to(x.device) # add positional encoding to the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6c2c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step2: scaled dot product attention\n",
    "def sclaed_dot_product_attention(q, k, v, mask=None):\n",
    "    # q, k, v shape (batch_size, head, seq_len, depth = d_k = d_model/num_heads)\n",
    "    d_k = q.size(-1) # get the last dimention size, which is the depth of the each head\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) /math.sqrt(d_k) # calculate the attention scores\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf')) # apply the mask to the scores\n",
    "    \n",
    "    atten = torch.softmax(scores, dim = -1) # apply softmax to the scores\n",
    "    output = torch.matmul(atten, v) # calculate the output\n",
    "    return output # output shape (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f25312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step3: Multi-head attention imlementation\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # projection matrices for Q, K and V and output\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # q, k, v shape (batch_size, seq_len, d_model)\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        # linear projection and reshape into (batch_size, num_heads, seq_len, d_k)\n",
    "        q = self.q_linear(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2) # shape (batch_size, num_heads, seq_len, d_k) # calulates Q=x⋅WQ+bQ and perform reshape\n",
    "        k = self.k_linear(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
    "        v = self.v_linear(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
    "\n",
    "        # apply scaled dot product attention\n",
    "        attn = sclaed_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        # concatenate heads and project back\n",
    "        attn = attn.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "        output = self.out_linear(attn) # shape (batch_size, seq_len, d_model)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b2d8e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step4: Position-wise feed-forward network implementation\n",
    "# FFN is shared accross all positions, but applied individually to each token. This is called position-wise feed-forward network.\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FeedForward(nn.Module): # input and output dimension are same but is transformed by a linear layer/ It helps transform the attended info non-linearly before going to the next layer\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff) # expand\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x))) # apply ReLU (adds non-linearity) activation and then linear transformation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ce1cebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 3, 4])\n",
      "First token transformed: tensor([-1.6731, -1.2507,  0.1889, -0.6106], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "d_model = 4\n",
    "d_ff = 8  # Hidden layer size (usually 4×d_model in real Transformer)\n",
    "\n",
    "# Sample input (2 sequences of 3 tokens, each with embedding size 4)\n",
    "x = torch.tensor([\n",
    "    [[1.0, 2.0, 3.0, 4.0],\n",
    "     [4.0, 3.0, 2.0, 1.0],\n",
    "     [0.0, 1.0, 0.0, 1.0]],\n",
    "     \n",
    "    [[1.0, 1.0, 1.0, 1.0],\n",
    "     [2.0, 2.0, 2.0, 2.0],\n",
    "     [3.0, 3.0, 3.0, 3.0]],\n",
    "])\n",
    "\n",
    "# Initialize and apply FFN\n",
    "ffn = FeedForward(d_model=d_model, d_ff=d_ff)\n",
    "output = ffn(x)\n",
    "\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"First token transformed:\", output[0, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b031387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step5: Trasformer encoder layer implementaion\n",
    "# here multihead attention - learn dependencies between tokens in the sequence\n",
    "# residual connections - help gradients flow through the network(prevent vanishing gradients problems)\n",
    "# normalization - stabilize the training process and improve convergence ( nomalize features for stable learning)\n",
    "# feed-forward network - apply non-linear transformations to the attended information\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)  # Multi-head attention\n",
    "        self.ffn = FeedForward(d_model, d_ff)  # Position-wise feed-forward network\n",
    "        self.norm1 = nn.LayerNorm(d_model)  # Layer normalization\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)  # Dropout for attention\n",
    "        self.dropout2 = nn.Dropout(dropout)  # Dropout for feed-forward network\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # 1. self-attention + residual connection + normalization\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x =  self.norm1(x + self.dropout1(attn_output)) # residual connection and normalization\n",
    "\n",
    "        # 2. FFN + residual connection + normalization\n",
    "        ffn_output = self.ffn(x) # FFN output\n",
    "        x = x + self.dropout2(ffn_output) # residual connection\n",
    "        x = self.norm2(x) # normalization \n",
    "\n",
    "        return x  # Output shape (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9fff72",
   "metadata": {},
   "source": [
    "## EncoderLayer Block Diagram\n",
    "\n",
    "```\n",
    "Input: x (batch_size, seq_len, d_model)\n",
    "                    ↓\n",
    "    ┌─────────────────────────────────────────┐\n",
    "    │         MULTI-HEAD ATTENTION            │\n",
    "    │  Query = x, Key = x, Value = x          │\n",
    "    │  attn_output = self_attn(x, x, x, mask) │\n",
    "    └─────────────────────────────────────────┘\n",
    "                    ↓\n",
    "              [Dropout1 Applied]\n",
    "                    ↓\n",
    "    ┌─────────────────────────────────────────┐\n",
    "    │         RESIDUAL + LAYER NORM           │\n",
    "    │    x = norm1(x + dropout1(attn_output)) │\n",
    "    └─────────────────────────────────────────┘\n",
    "                    ↓\n",
    "    ┌─────────────────────────────────────────┐\n",
    "    │         FEED FORWARD NETWORK            │\n",
    "    │      ffn_output = ffn(x)                │\n",
    "    │  (Linear → ReLU → Linear transformation)│\n",
    "    └─────────────────────────────────────────┘\n",
    "                    ↓\n",
    "              [Dropout2 Applied]\n",
    "                    ↓\n",
    "    ┌─────────────────────────────────────────┐\n",
    "    │         RESIDUAL + LAYER NORM           │\n",
    "    │    x = norm2(x + dropout2(ffn_output))  │\n",
    "    └─────────────────────────────────────────┘\n",
    "                    ↓\n",
    "Output: x (batch_size, seq_len, d_model)\n",
    "```\n",
    "\n",
    "### Component Breakdown:\n",
    "\n",
    "| Component | Purpose | Implementation Details |\n",
    "|-----------|---------|----------------------|\n",
    "| **Multi-Head Attention** | Learn token dependencies | `self_attn(x, x, x, mask)` - Self-attention with Q=K=V=x |\n",
    "| **Dropout1** | Regularization | `dropout1(attn_output)` - Prevent overfitting |\n",
    "| **Residual + LayerNorm** | Gradient flow & stability | `norm1(x + dropout1(attn_output))` - Add input to output |\n",
    "| **Feed Forward** | Non-linear transformation | `ffn(x)` - Linear → ReLU → Linear layers |\n",
    "| **Dropout2** | Regularization | `dropout2(ffn_output)` - Prevent overfitting |\n",
    "| **Residual + LayerNorm** | Gradient flow & stability | `norm2(x + dropout2(ffn_output))` - Add input to output |\n",
    "\n",
    "### Key Features:\n",
    "- **Two Sub-layers**: Self-attention + Feed-forward network\n",
    "- **Two Residual Connections**: Help gradients flow through deep networks\n",
    "- **Two Layer Normalizations**: Stabilize training and improve convergence\n",
    "- **Two Dropout Layers**: Regularization to prevent overfitting\n",
    "- **Consistent Shape**: Input and output have same dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a82406ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step6: Full Encoder block implementation\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)  # token embeddings\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len)  # positional encodings\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x: input token IDs → shape (batch_size, seq_len)\n",
    "        x = self.embedding(x) * math.sqrt(x.size(-1))  # shape: (batch, seq_len, d_model)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return x  # shape: (batch_size, seq_len, d_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90334d3d",
   "metadata": {},
   "source": [
    "## Transformer Decoder Architecture Flow\n",
    "\n",
    "```\n",
    "Input (shifted target embeddings)\n",
    "              ↓\n",
    "┌─────────────────────────────────┐\n",
    "│   Masked Multi-Head Self-Attn  │\n",
    "└─────────────────────────────────┘\n",
    "              ↓\n",
    "     + Residual → LayerNorm\n",
    "              ↓\n",
    "┌─────────────────────────────────┐\n",
    "│   Encoder-Decoder Cross-Attn   │\n",
    "└─────────────────────────────────┘\n",
    "              ↓\n",
    "     + Residual → LayerNorm\n",
    "              ↓\n",
    "┌─────────────────────────────────┐\n",
    "│         Feed Forward            │\n",
    "└─────────────────────────────────┘\n",
    "              ↓\n",
    "     + Residual → LayerNorm\n",
    "              ↓\n",
    "            Output\n",
    "```\n",
    "\n",
    "**Alternative Compact Version:**\n",
    "\n",
    "| Step | Component | Operation |\n",
    "|------|-----------|-----------|\n",
    "| 1 | **Input** | Shifted target embeddings |\n",
    "| 2 | **Self-Attention** | Masked Multi-Head Self-Attention |\n",
    "| 3 | **Residual** | Add & LayerNorm |\n",
    "| 4 | **Cross-Attention** | Encoder-Decoder Cross-Attention |\n",
    "| 5 | **Residual** | Add & LayerNorm |\n",
    "| 6 | **Feed Forward** | Position-wise Feed Forward |\n",
    "| 7 | **Residual** | Add & LayerNorm |\n",
    "| 8 | **Output** | Final decoder output |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da03abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step7: Transformer Decoder Layer Implementation\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, tgt_mask=None, memory_mask=None):\n",
    "        # Step 1: Masked self-attention\n",
    "        _x = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout1(_x))\n",
    "\n",
    "        # Step 2: Cross-attention with encoder output as key/value\n",
    "        _x = self.cross_attn(x, enc_output, enc_output, memory_mask)\n",
    "        x = self.norm2(x + self.dropout2(_x))\n",
    "\n",
    "        # Step 3: Feed-forward\n",
    "        _x = self.ffn(x)\n",
    "        x = self.norm3(x + self.dropout3(_x))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44761665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step8: Full Transformer Decoder Stack Implementation\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        # tgt shape: (batch_size, tgt_seq_len)\n",
    "        x = self.embedding(tgt) * math.sqrt(tgt.size(-1))\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, tgt_mask, memory_mask)\n",
    "\n",
    "        return x  # shape: (batch_size, tgt_seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f96cb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step9: Full Transformer Model (Encoder + Decoder)\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8, d_ff=2048, \n",
    "                 num_layers=6, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(src_vocab_size, d_model, num_heads, d_ff, num_layers, max_len, dropout)\n",
    "        self.decoder = TransformerDecoder(tgt_vocab_size, d_model, num_heads, d_ff, num_layers, max_len, dropout)\n",
    "        self.output_layer = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None):\n",
    "        memory = self.encoder(src, src_mask)              # Encoder output\n",
    "        dec_output = self.decoder(tgt, memory, tgt_mask, memory_mask)  # Decoder output\n",
    "        logits = self.output_layer(dec_output)            # Final token logits\n",
    "        return logits  # shape: (batch_size, tgt_seq_len, tgt_vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d8373bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Source: Project Gutenberg', 'Source: http://www.zeno.org - Contumax GmbH & Co. KG'), ('Jane Eyre', 'Jane Eyre'), ('Charlotte Bronte', 'Charlotte Bronte'), ('CHAPTER I', 'Erstes Kapitel'), ('There was no possibility of taking a walk that day.', 'Es war ganz unmöglich, an diesem Tage einen Spaziergang zu machen.')]\n"
     ]
    }
   ],
   "source": [
    "# Exmaple model usage\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"opus_books\", \"de-en\")\n",
    "train_data = dataset['train']\n",
    "\n",
    "examples = []\n",
    "for i in range(5):\n",
    "    ex = train_data[i]\n",
    "    src = ex['translation']['en']   \n",
    "    tgt = ex['translation']['de']\n",
    "    examples.append((src, tgt))\n",
    "\n",
    "print(examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcc0e16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 43\n"
     ]
    }
   ],
   "source": [
    "# tokenize + vocab building\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "specials = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
    "\n",
    "tokens = set()\n",
    "for src, tgt in examples:\n",
    "    tokens.update(tokenize(src))\n",
    "    tokens.update(tokenize(tgt))\n",
    "\n",
    "vocab = {tok: i for i, tok in enumerate(specials + sorted(tokens))}\n",
    "inv_vocab = {i: tok for tok, i in vocab.items()}\n",
    "print(\"Vocab size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa8a8e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_batch shape: torch.Size([5, 20])\n"
     ]
    }
   ],
   "source": [
    "# Encode + Pad \n",
    "def encode(text):\n",
    "    return [vocab['<sos>']] + [vocab.get(tok, vocab['<unk>']) for tok in tokenize(text)] + [vocab['<eos>']]\n",
    "\n",
    "def pad(seq, max_len):\n",
    "    return seq + [vocab['<pad>']] * (max_len - len(seq))\n",
    "\n",
    "src_max_len = 20\n",
    "tgt_max_len = 20\n",
    "\n",
    "src_encoded, tgt_encoded = [], []\n",
    "for src, tgt in examples:\n",
    "    src_encoded.append(pad(encode(src), src_max_len))\n",
    "    tgt_encoded.append(pad(encode(tgt), tgt_max_len))\n",
    "\n",
    "src_batch = torch.tensor(src_encoded).to(device)\n",
    "tgt_batch = torch.tensor(tgt_encoded).to(device)\n",
    "tgt_input = tgt_batch[:, :-1]\n",
    "tgt_output = tgt_batch[:, 1:]\n",
    "\n",
    "print(\"src_batch shape:\", src_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42143d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 3.7871\n",
      "Epoch 2 | Loss: 2.9557\n",
      "Epoch 3 | Loss: 3.2406\n",
      "Epoch 4 | Loss: 2.7756\n",
      "Epoch 5 | Loss: 2.2667\n",
      "Epoch 6 | Loss: 1.8530\n",
      "Epoch 7 | Loss: 2.2611\n",
      "Epoch 8 | Loss: 3.0001\n",
      "Epoch 9 | Loss: 1.0404\n",
      "Epoch 10 | Loss: 0.9462\n",
      "Epoch 11 | Loss: 2.5405\n",
      "Epoch 12 | Loss: 1.6173\n",
      "Epoch 13 | Loss: 0.9900\n",
      "Epoch 14 | Loss: 1.2688\n",
      "Epoch 15 | Loss: 0.7826\n",
      "Epoch 16 | Loss: 0.5715\n",
      "Epoch 17 | Loss: 0.3749\n",
      "Epoch 18 | Loss: 0.2788\n",
      "Epoch 19 | Loss: 0.3029\n",
      "Epoch 20 | Loss: 0.1772\n",
      "Epoch 21 | Loss: 0.1501\n",
      "Epoch 22 | Loss: 0.1455\n",
      "Epoch 23 | Loss: 0.1863\n",
      "Epoch 24 | Loss: 0.1097\n",
      "Epoch 25 | Loss: 0.1202\n",
      "Epoch 26 | Loss: 0.0480\n",
      "Epoch 27 | Loss: 0.0275\n",
      "Epoch 28 | Loss: 0.0506\n",
      "Epoch 29 | Loss: 0.0408\n",
      "Epoch 30 | Loss: 0.0354\n"
     ]
    }
   ],
   "source": [
    "# Model traiing \n",
    "model = Transformer(\n",
    "    src_vocab_size=len(vocab),\n",
    "    tgt_vocab_size=len(vocab),\n",
    "    d_model=128,\n",
    "    num_heads=8,\n",
    "    d_ff=512,\n",
    "    num_layers=2,\n",
    ").to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(30):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(src_batch, tgt_input)\n",
    "    loss = criterion(logits.view(-1, len(vocab)), tgt_output.reshape(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1} | Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9dfa2e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Source: Project Gutenberg\n",
      "Predicted: ['charlotte', 'bronte', 'bronte', 'bronte', 'bronte', 'bronte', 'bronte', 'bronte', 'bronte', 'bronte', 'bronte', 'bronte', 'bronte', 'bronte', 'bronte']\n"
     ]
    }
   ],
   "source": [
    "# Greedy Decoding for Inference\n",
    "def greedy_decode(model, src_seq, max_len=15):\n",
    "    model.eval()\n",
    "    src = torch.tensor([src_seq], device=device)\n",
    "    memory = model.encoder(src)\n",
    "    ys = torch.tensor([[vocab['<sos>']]], device=device)\n",
    "    for _ in range(max_len):\n",
    "        out = model.decoder(ys, memory)\n",
    "        logits = model.output_layer(out[:, -1:])\n",
    "        next_token = torch.argmax(logits, dim=-1)[:, -1].unsqueeze(1)\n",
    "        ys = torch.cat([ys, next_token], dim=1)\n",
    "        if next_token.item() == vocab['<eos>']:\n",
    "            break\n",
    "    return ys.squeeze().tolist()\n",
    "src_test = pad(encode(examples[0][0]), src_max_len)\n",
    "out_ids = greedy_decode(model, src_test)\n",
    "print(\"Input:\", examples[0][0])\n",
    "print(\"Predicted:\", [inv_vocab[i] for i in out_ids if inv_vocab[i] not in specials])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b0c80c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch_cuda)",
   "language": "python",
   "name": "torch_cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
