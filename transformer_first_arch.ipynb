{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85402ece",
   "metadata": {},
   "source": [
    "## Transformer architecture from attention is all you need paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7c230dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step1:Positional Encoding implementation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # create a matrix of shape (max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # shape (0, max_len) ----> shape (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # captures the positional relationships \n",
    "\n",
    "        # fill the positional encoding matrix with sine for even  and consine for odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) \n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # add batch dimension for broadcasting\n",
    "        pe = pe.unsqueeze(0) # shape (1, max_len, d_model)\n",
    "        \n",
    "        # Register as buffer so it's saved with the model but not considered a parameter\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape (batch_size, seq_len, d_model)\n",
    "        # get the positional encoding for the input sequence length\n",
    "        return x + self.pe[:, :x.size(1), :].to(x.device) # add positional encoding to the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e6c2c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step2: scaled dot product attention\n",
    "def sclaed_dot_product_attention(q, k, v, mask=None):\n",
    "    # q, k, v shape (batch_size, head, seq_len, depth = d_k = d_model/num_heads)\n",
    "    d_k = q.size(-1) # get the last dimention size, which is the depth of the each head\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) /math.sqrt(d_k) # calculate the attention scores\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf')) # apply the mask to the scores\n",
    "    \n",
    "    atten = torch.softmax(scores, dim = -1) # apply softmax to the scores\n",
    "    output = torch.matmul(atten, v) # calculate the output\n",
    "    return output # output shape (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6f25312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step3: Multi-head attention imlementation\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # projection matrices for Q, K and V and output\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # q, k, v shape (batch_size, seq_len, d_model)\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        # linear projection and reshape into (batch_size, num_heads, seq_len, d_k)\n",
    "        q = self.q_linear(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2) # shape (batch_size, num_heads, seq_len, d_k) # calulates Q=x⋅WQ+bQ and perform reshape\n",
    "        k = self.k_linear(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
    "        v = self.v_linear(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
    "\n",
    "        # apply scaled dot product attention\n",
    "        attn = sclaed_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        # concatenate heads and project back\n",
    "        attn = attn.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "        output = self.out_linear(attn) # shape (batch_size, seq_len, d_model)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7b2d8e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step4: Position-wise feed-forward network implementation\n",
    "# FFN is shared accross all positions, but applied individually to each token. This is called position-wise feed-forward network.\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FeedForward(nn.Module): # input and output dimension are same but is transformed by a linear layer/ It helps transform the attended info non-linearly before going to the next layer\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff) # expand\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x))) # apply ReLU (adds non-linearity) activation and then linear transformation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7ce1cebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 3, 4])\n",
      "First token transformed: tensor([ 0.2016, -0.1959,  0.4809, -0.4135], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "d_model = 4\n",
    "d_ff = 8  # Hidden layer size (usually 4×d_model in real Transformer)\n",
    "\n",
    "# Sample input (2 sequences of 3 tokens, each with embedding size 4)\n",
    "x = torch.tensor([\n",
    "    [[1.0, 2.0, 3.0, 4.0],\n",
    "     [4.0, 3.0, 2.0, 1.0],\n",
    "     [0.0, 1.0, 0.0, 1.0]],\n",
    "     \n",
    "    [[1.0, 1.0, 1.0, 1.0],\n",
    "     [2.0, 2.0, 2.0, 2.0],\n",
    "     [3.0, 3.0, 3.0, 3.0]],\n",
    "])\n",
    "\n",
    "# Initialize and apply FFN\n",
    "ffn = FeedForward(d_model=d_model, d_ff=d_ff)\n",
    "output = ffn(x)\n",
    "\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"First token transformed:\", output[0, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2b031387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step5: Trasformer encoder layer implementaion\n",
    "# here multihead attention - learn dependencies between tokens in the sequence\n",
    "# residual connections - help gradients flow through the network(prevent vanishing gradients problems)\n",
    "# normalization - stabilize the training process and improve convergence ( nomalize features for stable learning)\n",
    "# feed-forward network - apply non-linear transformations to the attended information\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)  # Multi-head attention\n",
    "        self.ffn = FeedForward(d_model, d_ff)  # Position-wise feed-forward network\n",
    "        self.norm1 = nn.LayerNorm(d_model)  # Layer normalization\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)  # Dropout for attention\n",
    "        self.dropout2 = nn.Dropout(dropout)  # Dropout for feed-forward network\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # 1. self-attention + residual connection + normalization\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x =  self.norm1(x + self.dropout1(attn_output)) # residual connection and normalization\n",
    "\n",
    "        # 2. FFN + residual connection + normalization\n",
    "        ffn_output = self.ffn(x) # FFN output\n",
    "        x = x + self.dropout2(ffn_output) # residual connection\n",
    "        x = self.norm2(x) # normalization \n",
    "\n",
    "        return x  # Output shape (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9fff72",
   "metadata": {},
   "source": [
    "## EncoderLayer Block Diagram\n",
    "\n",
    "```\n",
    "Input: x (batch_size, seq_len, d_model)\n",
    "                    ↓\n",
    "    ┌─────────────────────────────────────────┐\n",
    "    │         MULTI-HEAD ATTENTION            │\n",
    "    │  Query = x, Key = x, Value = x          │\n",
    "    │  attn_output = self_attn(x, x, x, mask) │\n",
    "    └─────────────────────────────────────────┘\n",
    "                    ↓\n",
    "              [Dropout1 Applied]\n",
    "                    ↓\n",
    "    ┌─────────────────────────────────────────┐\n",
    "    │         RESIDUAL + LAYER NORM           │\n",
    "    │    x = norm1(x + dropout1(attn_output)) │\n",
    "    └─────────────────────────────────────────┘\n",
    "                    ↓\n",
    "    ┌─────────────────────────────────────────┐\n",
    "    │         FEED FORWARD NETWORK            │\n",
    "    │      ffn_output = ffn(x)                │\n",
    "    │  (Linear → ReLU → Linear transformation)│\n",
    "    └─────────────────────────────────────────┘\n",
    "                    ↓\n",
    "              [Dropout2 Applied]\n",
    "                    ↓\n",
    "    ┌─────────────────────────────────────────┐\n",
    "    │         RESIDUAL + LAYER NORM           │\n",
    "    │    x = norm2(x + dropout2(ffn_output))  │\n",
    "    └─────────────────────────────────────────┘\n",
    "                    ↓\n",
    "Output: x (batch_size, seq_len, d_model)\n",
    "```\n",
    "\n",
    "### Component Breakdown:\n",
    "\n",
    "| Component | Purpose | Implementation Details |\n",
    "|-----------|---------|----------------------|\n",
    "| **Multi-Head Attention** | Learn token dependencies | `self_attn(x, x, x, mask)` - Self-attention with Q=K=V=x |\n",
    "| **Dropout1** | Regularization | `dropout1(attn_output)` - Prevent overfitting |\n",
    "| **Residual + LayerNorm** | Gradient flow & stability | `norm1(x + dropout1(attn_output))` - Add input to output |\n",
    "| **Feed Forward** | Non-linear transformation | `ffn(x)` - Linear → ReLU → Linear layers |\n",
    "| **Dropout2** | Regularization | `dropout2(ffn_output)` - Prevent overfitting |\n",
    "| **Residual + LayerNorm** | Gradient flow & stability | `norm2(x + dropout2(ffn_output))` - Add input to output |\n",
    "\n",
    "### Key Features:\n",
    "- **Two Sub-layers**: Self-attention + Feed-forward network\n",
    "- **Two Residual Connections**: Help gradients flow through deep networks\n",
    "- **Two Layer Normalizations**: Stabilize training and improve convergence\n",
    "- **Two Dropout Layers**: Regularization to prevent overfitting\n",
    "- **Consistent Shape**: Input and output have same dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a82406ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step6: Full Encoder block implementation\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)  # token embeddings\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len)  # positional encodings\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x: input token IDs → shape (batch_size, seq_len)\n",
    "        x = self.embedding(x) * math.sqrt(x.size(-1))  # shape: (batch, seq_len, d_model)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return x  # shape: (batch_size, seq_len, d_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90334d3d",
   "metadata": {},
   "source": [
    "## Transformer Decoder Architecture Flow\n",
    "\n",
    "```\n",
    "Input (shifted target embeddings)\n",
    "              ↓\n",
    "┌─────────────────────────────────┐\n",
    "│   Masked Multi-Head Self-Attn  │\n",
    "└─────────────────────────────────┘\n",
    "              ↓\n",
    "     + Residual → LayerNorm\n",
    "              ↓\n",
    "┌─────────────────────────────────┐\n",
    "│   Encoder-Decoder Cross-Attn   │\n",
    "└─────────────────────────────────┘\n",
    "              ↓\n",
    "     + Residual → LayerNorm\n",
    "              ↓\n",
    "┌─────────────────────────────────┐\n",
    "│         Feed Forward            │\n",
    "└─────────────────────────────────┘\n",
    "              ↓\n",
    "     + Residual → LayerNorm\n",
    "              ↓\n",
    "            Output\n",
    "```\n",
    "\n",
    "**Alternative Compact Version:**\n",
    "\n",
    "| Step | Component | Operation |\n",
    "|------|-----------|-----------|\n",
    "| 1 | **Input** | Shifted target embeddings |\n",
    "| 2 | **Self-Attention** | Masked Multi-Head Self-Attention |\n",
    "| 3 | **Residual** | Add & LayerNorm |\n",
    "| 4 | **Cross-Attention** | Encoder-Decoder Cross-Attention |\n",
    "| 5 | **Residual** | Add & LayerNorm |\n",
    "| 6 | **Feed Forward** | Position-wise Feed Forward |\n",
    "| 7 | **Residual** | Add & LayerNorm |\n",
    "| 8 | **Output** | Final decoder output |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "da03abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step7: Transformer Decoder Layer Implementation\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, tgt_mask=None, memory_mask=None):\n",
    "        # Step 1: Masked self-attention\n",
    "        _x = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout1(_x))\n",
    "\n",
    "        # Step 2: Cross-attention with encoder output as key/value\n",
    "        _x = self.cross_attn(x, enc_output, enc_output, memory_mask)\n",
    "        x = self.norm2(x + self.dropout2(_x))\n",
    "\n",
    "        # Step 3: Feed-forward\n",
    "        _x = self.ffn(x)\n",
    "        x = self.norm3(x + self.dropout3(_x))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "44761665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step8: Full Transformer Decoder Stack Implementation\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        # tgt shape: (batch_size, tgt_seq_len)\n",
    "        x = self.embedding(tgt) * math.sqrt(tgt.size(-1))\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, tgt_mask, memory_mask)\n",
    "\n",
    "        return x  # shape: (batch_size, tgt_seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8f96cb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step9: Full Transformer Model (Encoder + Decoder)\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8, d_ff=2048, \n",
    "                 num_layers=6, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(src_vocab_size, d_model, num_heads, d_ff, num_layers, max_len, dropout)\n",
    "        self.decoder = TransformerDecoder(tgt_vocab_size, d_model, num_heads, d_ff, num_layers, max_len, dropout)\n",
    "        self.output_layer = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None):\n",
    "        memory = self.encoder(src, src_mask)              # Encoder output\n",
    "        dec_output = self.decoder(tgt, memory, tgt_mask, memory_mask)  # Decoder output\n",
    "        logits = self.output_layer(dec_output)            # Final token logits\n",
    "        return logits  # shape: (batch_size, tgt_seq_len, tgt_vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "03209861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Training and testing the Transformer model\n",
    "\n",
    "# Manual toy dataset\n",
    "raw_data = [\n",
    "    (\"I am a student\", \"Ich bin ein Schüler\"),\n",
    "    (\"He is a teacher\", \"Er ist ein Lehrer\"),\n",
    "    (\"She is reading a book\", \"Sie liest ein Buch\"),\n",
    "    (\"They are playing\", \"Sie spielen\"),\n",
    "    (\"We love learning\", \"Wir lieben das Lernen\"),\n",
    "]\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return sentence.lower().split()\n",
    "\n",
    "specials = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
    "\n",
    "# Build vocab\n",
    "def build_vocab(sentences):\n",
    "    tokens = set()\n",
    "    for s in sentences:\n",
    "        tokens.update(tokenize(s))\n",
    "    return {tok: i for i, tok in enumerate(specials + sorted(tokens))}\n",
    "\n",
    "# Tokenize and vocab\n",
    "src_vocab = build_vocab([src for src, _ in raw_data])\n",
    "tgt_vocab = build_vocab([tgt for _, tgt in raw_data])\n",
    "inv_tgt_vocab = {v: k for k, v in tgt_vocab.items()}\n",
    "\n",
    "# Encode sentences using the vocab\n",
    "# Add <sos> at the beginning and <eos> at the end of each\n",
    "# sentence, and replace unknown tokens with <unk>\n",
    "# Also, pad sequences to a fixed length\n",
    "# with <pad> token\n",
    "# The <sos> and <eos> tokens are used to indicate the start and end of a sequence, respectively.\n",
    "# The <unk> token is used for unknown words that are not in the vocabulary.\n",
    "# The <pad> token is used to pad sequences to a fixed length\n",
    "# The vocabulary is built from the sentences, and each token is assigned a unique index.\n",
    "# The encode function converts a sentence into a sequence of indices based on the vocabulary.\n",
    "# The pad function pads the sequence to a fixed length with the <pad> token.\n",
    "# The src_vocab and tgt_vocab dictionaries map tokens to their corresponding indices.\n",
    "def encode(sentence, vocab):\n",
    "    return [vocab['<sos>']] + [vocab.get(tok, vocab['<unk>']) for tok in tokenize(sentence)] + [vocab['<eos>']]\n",
    "\n",
    "def pad(seq, max_len, pad_id):\n",
    "    return seq + [pad_id] * (max_len - len(seq))\n",
    "\n",
    "src_encoded = []\n",
    "tgt_encoded = []\n",
    "\n",
    "max_src_len = 10\n",
    "max_tgt_len = 12\n",
    "\n",
    "for src, tgt in raw_data:\n",
    "    src_seq = pad(encode(src, src_vocab), max_src_len, src_vocab['<pad>'])\n",
    "    tgt_seq = pad(encode(tgt, tgt_vocab), max_tgt_len, tgt_vocab['<pad>'])\n",
    "    src_encoded.append(src_seq)\n",
    "    tgt_encoded.append(tgt_seq)\n",
    "\n",
    "import torch\n",
    "src_batch = torch.tensor(src_encoded)\n",
    "tgt_batch = torch.tensor(tgt_encoded)\n",
    "\n",
    "# For training\n",
    "tgt_input = tgt_batch[:, :-1]\n",
    "tgt_output = tgt_batch[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "54b7528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model, Loss, and Optimizer \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Use CPU to avoid CUDA device interface issues\n",
    "device = torch.device(\"cpu\")  # Force CPU for stable execution\n",
    "\n",
    "vocab_size_src = len(src_vocab)\n",
    "vocab_size_tgt = len(tgt_vocab)\n",
    "\n",
    "# Initialize model\n",
    "model = Transformer(\n",
    "    src_vocab_size=vocab_size_src,\n",
    "    tgt_vocab_size=vocab_size_tgt,\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    d_ff=2048,\n",
    "    num_layers=2,\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "77e5a5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/grad17/nrai/miniconda3/envs/ml_project/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_21850/2478532165.py\", line 21, in <module>\n",
      "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
      "  File \"/home/grad17/nrai/miniconda3/envs/ml_project/lib/python3.9/site-packages/torch/optim/sgd.py\", line 63, in __init__\n",
      "    if fused:\n",
      "  File \"/home/grad17/nrai/miniconda3/envs/ml_project/lib/python3.9/site-packages/torch/optim/optimizer.py\", line 377, in __init__\n",
      "  File \"/home/grad17/nrai/miniconda3/envs/ml_project/lib/python3.9/site-packages/torch/_compile.py\", line 27, in inner\n",
      "    import torch._dynamo\n",
      "  File \"/home/grad17/nrai/miniconda3/envs/ml_project/lib/python3.9/site-packages/torch/_dynamo/__init__.py\", line 3, in <module>\n",
      "    from . import convert_frame, eval_frame, resume_execution\n",
      "  File \"/home/grad17/nrai/miniconda3/envs/ml_project/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py\", line 53, in <module>\n",
      "    from . import config, exc, trace_rules\n",
      "  File \"/home/grad17/nrai/miniconda3/envs/ml_project/lib/python3.9/site-packages/torch/_dynamo/trace_rules.py\", line 46, in <module>\n",
      "    from .variables import (\n",
      "  File \"/home/grad17/nrai/miniconda3/envs/ml_project/lib/python3.9/site-packages/torch/_dynamo/variables/__init__.py\", line 2, in <module>\n",
      "    from .builtin import BuiltinVariable\n",
      "  File \"/home/grad17/nrai/miniconda3/envs/ml_project/lib/python3.9/site-packages/torch/_dynamo/variables/builtin.py\", line 47, in <module>\n",
      "    from .ctx_manager import EventVariable, StreamVariable\n",
      "  File \"/home/grad17/nrai/miniconda3/envs/ml_project/lib/python3.9/site-packages/torch/_dynamo/variables/ctx_manager.py\", line 17, in <module>\n",
      "    from ..device_interface import get_interface_for_device\n",
      "  File \"/home/grad17/nrai/miniconda3/envs/ml_project/lib/python3.9/site-packages/torch/_dynamo/device_interface.py\", line 154, in <module>\n",
      "    class CudaInterface(DeviceInterface):\n",
      "  File \"/home/grad17/nrai/miniconda3/envs/ml_project/lib/python3.9/site-packages/torch/_dynamo/device_interface.py\", line 26, in __new__\n",
      "    assert inspect.isclass(class_member[\"Event\"]) and issubclass(\n",
      "AssertionError: DeviceInterface member Event should be inherit from _EventBase\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/grad17/nrai/miniconda3/envs/ml_project/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/grad17/nrai/miniconda3/envs/ml_project/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/grad17/nrai/miniconda3/envs/ml_project/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/grad17/nrai/miniconda3/envs/ml_project/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/grad17/nrai/miniconda3/envs/ml_project/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/home/grad17/nrai/miniconda3/envs/ml_project/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/home/grad17/nrai/miniconda3/envs/ml_project/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/home/grad17/nrai/miniconda3/envs/ml_project/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/grad17/nrai/miniconda3/envs/ml_project/lib/python3.9/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home/grad17/nrai/miniconda3/envs/ml_project/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/grad17/nrai/miniconda3/envs/ml_project/lib/python3.9/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/home/grad17/nrai/miniconda3/envs/ml_project/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/grad17/nrai/miniconda3/envs/ml_project/lib/python3.9/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/home/grad17/nrai/miniconda3/envs/ml_project/lib/python3.9/site-packages/executing/executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vocab_size_src = len(src_vocab)\n",
    "vocab_size_tgt = len(tgt_vocab)\n",
    "\n",
    "# Initialize model\n",
    "model = Transformer(\n",
    "    src_vocab_size=vocab_size_src,\n",
    "    tgt_vocab_size=vocab_size_tgt,\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    d_ff=2048,\n",
    "    num_layers=2,\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab['<pad>'])\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296dd979",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c694986e",
   "metadata": {},
   "source": [
    "## EncoderLayer Block Diagram\n",
    "\n",
    "```\n",
    "Input: x (batch_size, seq_len, d_model)\n",
    "                    ↓\n",
    "    ┌─────────────────────────────────────────┐\n",
    "    │         MULTI-HEAD ATTENTION            │\n",
    "    │  Query = x, Key = x, Value = x          │\n",
    "    │  attn_output = self_attn(x, x, x, mask) │\n",
    "    └─────────────────────────────────────────┘\n",
    "                    ↓\n",
    "              [Dropout1 Applied]\n",
    "                    ↓\n",
    "    ┌─────────────────────────────────────────┐\n",
    "    │         RESIDUAL + LAYER NORM           │\n",
    "    │    x = norm1(x + dropout1(attn_output)) │\n",
    "    └─────────────────────────────────────────┘\n",
    "                    ↓\n",
    "    ┌─────────────────────────────────────────┐\n",
    "    │         FEED FORWARD NETWORK            │\n",
    "    │      ffn_output = ffn(x)                │\n",
    "    │  (Linear → ReLU → Linear transformation)│\n",
    "    └─────────────────────────────────────────┘\n",
    "                    ↓\n",
    "              [Dropout2 Applied]\n",
    "                    ↓\n",
    "    ┌─────────────────────────────────────────┐\n",
    "    │         RESIDUAL + LAYER NORM           │\n",
    "    │    x = norm2(x + dropout2(ffn_output))  │\n",
    "    └─────────────────────────────────────────┘\n",
    "                    ↓\n",
    "Output: x (batch_size, seq_len, d_model)\n",
    "```\n",
    "\n",
    "### Component Breakdown:\n",
    "\n",
    "| Component | Purpose | Implementation Details |\n",
    "|-----------|---------|----------------------|\n",
    "| **Multi-Head Attention** | Learn token dependencies | `self_attn(x, x, x, mask)` - Self-attention with Q=K=V=x |\n",
    "| **Dropout1** | Regularization | `dropout1(attn_output)` - Prevent overfitting |\n",
    "| **Residual + LayerNorm** | Gradient flow & stability | `norm1(x + dropout1(attn_output))` - Add input to output |\n",
    "| **Feed Forward** | Non-linear transformation | `ffn(x)` - Linear → ReLU → Linear layers |\n",
    "| **Dropout2** | Regularization | `dropout2(ffn_output)` - Prevent overfitting |\n",
    "| **Residual + LayerNorm** | Gradient flow & stability | `norm2(x + dropout2(ffn_output))` - Add input to output |\n",
    "\n",
    "### Key Features:\n",
    "- **Two Sub-layers**: Self-attention + Feed-forward network\n",
    "- **Two Residual Connections**: Help gradients flow through deep networks\n",
    "- **Two Layer Normalizations**: Stabilize training and improve convergence\n",
    "- **Two Dropout Layers**: Regularization to prevent overfitting\n",
    "- **Consistent Shape**: Input and output have same dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd1a2f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73ce18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check PyTorch version and system information\n",
    "import torch\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "print(\"🔍 PYTORCH & SYSTEM INFORMATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Architecture: {platform.architecture()}\")\n",
    "\n",
    "print(f\"\\n🖥️  DEVICE INFORMATION:\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    print(\"CUDA not available - using CPU\")\n",
    "\n",
    "print(f\"\\n⚙️  PYTORCH BUILD INFO:\")\n",
    "print(f\"PyTorch built with CUDA: {torch.version.cuda is not None}\")\n",
    "print(f\"PyTorch built with cuDNN: {torch.backends.cudnn.enabled}\")\n",
    "print(f\"MPS (Apple Silicon) available: {torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else 'N/A'}\")\n",
    "\n",
    "print(f\"\\n🧮 TENSOR BACKEND:\")\n",
    "print(f\"Default tensor type: {torch.get_default_dtype()}\")\n",
    "print(f\"Current device: {device}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_project)",
   "language": "python",
   "name": "ml_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
