{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85402ece",
   "metadata": {},
   "source": [
    "## Transformer architecture from attention is all you need paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7c230dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step1:Positional Encoding implementation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # create a matrix of shape (max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # shape (0, max_len) ----> shape (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # captures the positional relationships \n",
    "\n",
    "        # fill the positional encoding matrix with sine for even  and consine for odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) \n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # add batch dimension for broadcasting\n",
    "        pe = pe.unsqueeze(0) # shape (1, max_len, d_model)\n",
    "        \n",
    "        # Register as buffer so it's saved with the model but not considered a parameter\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape (batch_size, seq_len, d_model)\n",
    "        # get the positional encoding for the input sequence length\n",
    "        return x + self.pe[:, :x.size(1), :].to(x.device) # add positional encoding to the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e6c2c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step2: scaled dot product attention\n",
    "def sclaed_dot_product_attention(q, k, v, mask=None):\n",
    "    # q, k, v shape (batch_size, head, seq_len, depth = d_k = d_model/num_heads)\n",
    "    d_k = q.size(-1) # get the last dimention size, which is the depth of the each head\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) /math.sqrt(d_k) # calculate the attention scores\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf')) # apply the mask to the scores\n",
    "    \n",
    "    atten = torch.softmax(scores, dim = -1) # apply softmax to the scores\n",
    "    output = torch.matmul(atten, v) # calculate the output\n",
    "    return output # output shape (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6f25312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step3: Multi-head attention imlementation\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # projection matrices for Q, K and V and output\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # q, k, v shape (batch_size, seq_len, d_model)\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        # linear projection and reshape into (batch_size, num_heads, seq_len, d_k)\n",
    "        q = self.q_linear(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2) # shape (batch_size, num_heads, seq_len, d_k) # calulates Q=x⋅WQ+bQ and perform reshape\n",
    "        k = self.k_linear(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
    "        v = self.v_linear(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
    "\n",
    "        # apply scaled dot product attention\n",
    "        attn = sclaed_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        # concatenate heads and project back\n",
    "        attn = attn.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "        output = self.out_linear(attn) # shape (batch_size, seq_len, d_model)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7b2d8e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step4: Position-wise feed-forward network implementation\n",
    "# FFN is shared accross all positions, but applied individually to each token. This is called position-wise feed-forward network.\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FeedForward(nn.Module): # input and output dimension are same but is transformed by a linear layer/ It helps transform the attended info non-linearly before going to the next layer\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff) # expand\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x))) # apply ReLU (adds non-linearity) activation and then linear transformation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7ce1cebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 3, 4])\n",
      "First token transformed: tensor([ 0.3409, -1.7106,  0.0177, -0.8115], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "d_model = 4\n",
    "d_ff = 8  # Hidden layer size (usually 4×d_model in real Transformer)\n",
    "\n",
    "# Sample input (2 sequences of 3 tokens, each with embedding size 4)\n",
    "x = torch.tensor([\n",
    "    [[1.0, 2.0, 3.0, 4.0],\n",
    "     [4.0, 3.0, 2.0, 1.0],\n",
    "     [0.0, 1.0, 0.0, 1.0]],\n",
    "     \n",
    "    [[1.0, 1.0, 1.0, 1.0],\n",
    "     [2.0, 2.0, 2.0, 2.0],\n",
    "     [3.0, 3.0, 3.0, 3.0]],\n",
    "])\n",
    "\n",
    "# Initialize and apply FFN\n",
    "ffn = FeedForward(d_model=d_model, d_ff=d_ff)\n",
    "output = ffn(x)\n",
    "\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"First token transformed:\", output[0, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2b031387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step5: Trasformer encoder layer implementaion\n",
    "# here multihead attention - learn dependencies between tokens in the sequence\n",
    "# residual connections - help gradients flow through the network(prevent vanishing gradients problems)\n",
    "# normalization - stabilize the training process and improve convergence ( nomalize features for stable learning)\n",
    "# feed-forward network - apply non-linear transformations to the attended information\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)  # Multi-head attention\n",
    "        self.ffn = FeedForward(d_model, d_ff)  # Position-wise feed-forward network\n",
    "        self.norm1 = nn.LayerNorm(d_model)  # Layer normalization\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)  # Dropout for attention\n",
    "        self.dropout2 = nn.Dropout(dropout)  # Dropout for feed-forward network\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # 1. self-attention + residual connection + normalization\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x =  self.norm1(x + self.dropout1(attn_output)) # residual connection and normalization\n",
    "\n",
    "        # 2. FFN + residual connection + normalization\n",
    "        ffn_output = self.ffn(x) # FFN output\n",
    "        x = x + self.dropout2(ffn_output) # residual connection\n",
    "        x = self.norm2(x) # normalization \n",
    "\n",
    "        return x  # Output shape (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9fff72",
   "metadata": {},
   "source": [
    "## EncoderLayer Block Diagram\n",
    "\n",
    "```\n",
    "Input: x (batch_size, seq_len, d_model)\n",
    "                    ↓\n",
    "    ┌─────────────────────────────────────────┐\n",
    "    │         MULTI-HEAD ATTENTION            │\n",
    "    │  Query = x, Key = x, Value = x          │\n",
    "    │  attn_output = self_attn(x, x, x, mask) │\n",
    "    └─────────────────────────────────────────┘\n",
    "                    ↓\n",
    "              [Dropout1 Applied]\n",
    "                    ↓\n",
    "    ┌─────────────────────────────────────────┐\n",
    "    │         RESIDUAL + LAYER NORM           │\n",
    "    │    x = norm1(x + dropout1(attn_output)) │\n",
    "    └─────────────────────────────────────────┘\n",
    "                    ↓\n",
    "    ┌─────────────────────────────────────────┐\n",
    "    │         FEED FORWARD NETWORK            │\n",
    "    │      ffn_output = ffn(x)                │\n",
    "    │  (Linear → ReLU → Linear transformation)│\n",
    "    └─────────────────────────────────────────┘\n",
    "                    ↓\n",
    "              [Dropout2 Applied]\n",
    "                    ↓\n",
    "    ┌─────────────────────────────────────────┐\n",
    "    │         RESIDUAL + LAYER NORM           │\n",
    "    │    x = norm2(x + dropout2(ffn_output))  │\n",
    "    └─────────────────────────────────────────┘\n",
    "                    ↓\n",
    "Output: x (batch_size, seq_len, d_model)\n",
    "```\n",
    "\n",
    "### Component Breakdown:\n",
    "\n",
    "| Component | Purpose | Implementation Details |\n",
    "|-----------|---------|----------------------|\n",
    "| **Multi-Head Attention** | Learn token dependencies | `self_attn(x, x, x, mask)` - Self-attention with Q=K=V=x |\n",
    "| **Dropout1** | Regularization | `dropout1(attn_output)` - Prevent overfitting |\n",
    "| **Residual + LayerNorm** | Gradient flow & stability | `norm1(x + dropout1(attn_output))` - Add input to output |\n",
    "| **Feed Forward** | Non-linear transformation | `ffn(x)` - Linear → ReLU → Linear layers |\n",
    "| **Dropout2** | Regularization | `dropout2(ffn_output)` - Prevent overfitting |\n",
    "| **Residual + LayerNorm** | Gradient flow & stability | `norm2(x + dropout2(ffn_output))` - Add input to output |\n",
    "\n",
    "### Key Features:\n",
    "- **Two Sub-layers**: Self-attention + Feed-forward network\n",
    "- **Two Residual Connections**: Help gradients flow through deep networks\n",
    "- **Two Layer Normalizations**: Stabilize training and improve convergence\n",
    "- **Two Dropout Layers**: Regularization to prevent overfitting\n",
    "- **Consistent Shape**: Input and output have same dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a82406ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step6: Full Encoder block implementation\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)  # token embeddings\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len)  # positional encodings\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x: input token IDs → shape (batch_size, seq_len)\n",
    "        x = self.embedding(x) * math.sqrt(x.size(-1))  # shape: (batch, seq_len, d_model)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return x  # shape: (batch_size, seq_len, d_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90334d3d",
   "metadata": {},
   "source": [
    "## Transformer Decoder Architecture Flow\n",
    "\n",
    "```\n",
    "Input (shifted target embeddings)\n",
    "              ↓\n",
    "┌─────────────────────────────────┐\n",
    "│   Masked Multi-Head Self-Attn  │\n",
    "└─────────────────────────────────┘\n",
    "              ↓\n",
    "     + Residual → LayerNorm\n",
    "              ↓\n",
    "┌─────────────────────────────────┐\n",
    "│   Encoder-Decoder Cross-Attn   │\n",
    "└─────────────────────────────────┘\n",
    "              ↓\n",
    "     + Residual → LayerNorm\n",
    "              ↓\n",
    "┌─────────────────────────────────┐\n",
    "│         Feed Forward            │\n",
    "└─────────────────────────────────┘\n",
    "              ↓\n",
    "     + Residual → LayerNorm\n",
    "              ↓\n",
    "            Output\n",
    "```\n",
    "\n",
    "**Alternative Compact Version:**\n",
    "\n",
    "| Step | Component | Operation |\n",
    "|------|-----------|-----------|\n",
    "| 1 | **Input** | Shifted target embeddings |\n",
    "| 2 | **Self-Attention** | Masked Multi-Head Self-Attention |\n",
    "| 3 | **Residual** | Add & LayerNorm |\n",
    "| 4 | **Cross-Attention** | Encoder-Decoder Cross-Attention |\n",
    "| 5 | **Residual** | Add & LayerNorm |\n",
    "| 6 | **Feed Forward** | Position-wise Feed Forward |\n",
    "| 7 | **Residual** | Add & LayerNorm |\n",
    "| 8 | **Output** | Final decoder output |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "da03abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step7: Transformer Decoder Layer Implementation\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, tgt_mask=None, memory_mask=None):\n",
    "        # Step 1: Masked self-attention\n",
    "        _x = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout1(_x))\n",
    "\n",
    "        # Step 2: Cross-attention with encoder output as key/value\n",
    "        _x = self.cross_attn(x, enc_output, enc_output, memory_mask)\n",
    "        x = self.norm2(x + self.dropout2(_x))\n",
    "\n",
    "        # Step 3: Feed-forward\n",
    "        _x = self.ffn(x)\n",
    "        x = self.norm3(x + self.dropout3(_x))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "44761665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step8: Full Transformer Decoder Stack Implementation\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        # tgt shape: (batch_size, tgt_seq_len)\n",
    "        x = self.embedding(tgt) * math.sqrt(tgt.size(-1))\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, tgt_mask, memory_mask)\n",
    "\n",
    "        return x  # shape: (batch_size, tgt_seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8f96cb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step9: Full Transformer Model (Encoder + Decoder)\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8, d_ff=2048, \n",
    "                 num_layers=6, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(src_vocab_size, d_model, num_heads, d_ff, num_layers, max_len, dropout)\n",
    "        self.decoder = TransformerDecoder(tgt_vocab_size, d_model, num_heads, d_ff, num_layers, max_len, dropout)\n",
    "        self.output_layer = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None):\n",
    "        memory = self.encoder(src, src_mask)              # Encoder output\n",
    "        dec_output = self.decoder(tgt, memory, tgt_mask, memory_mask)  # Decoder output\n",
    "        logits = self.output_layer(dec_output)            # Final token logits\n",
    "        return logits  # shape: (batch_size, tgt_seq_len, tgt_vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "03209861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Training and testing the Transformer model\n",
    "\n",
    "# Manual toy dataset\n",
    "raw_data = [\n",
    "    (\"I am a student\", \"Ich bin ein Schüler\"),\n",
    "    (\"He is a teacher\", \"Er ist ein Lehrer\"),\n",
    "    (\"She is reading a book\", \"Sie liest ein Buch\"),\n",
    "    (\"They are playing\", \"Sie spielen\"),\n",
    "    (\"We love learning\", \"Wir lieben das Lernen\"),\n",
    "]\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return sentence.lower().split()\n",
    "\n",
    "specials = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
    "\n",
    "# Build vocab\n",
    "def build_vocab(sentences):\n",
    "    tokens = set()\n",
    "    for s in sentences:\n",
    "        tokens.update(tokenize(s))\n",
    "    return {tok: i for i, tok in enumerate(specials + sorted(tokens))}\n",
    "\n",
    "# Tokenize and vocab\n",
    "src_vocab = build_vocab([src for src, _ in raw_data])\n",
    "tgt_vocab = build_vocab([tgt for _, tgt in raw_data])\n",
    "inv_tgt_vocab = {v: k for k, v in tgt_vocab.items()}\n",
    "\n",
    "# Encode sentences using the vocab\n",
    "# Add <sos> at the beginning and <eos> at the end of each\n",
    "# sentence, and replace unknown tokens with <unk>\n",
    "# Also, pad sequences to a fixed length\n",
    "# with <pad> token\n",
    "# The <sos> and <eos> tokens are used to indicate the start and end of a sequence, respectively.\n",
    "# The <unk> token is used for unknown words that are not in the vocabulary.\n",
    "# The <pad> token is used to pad sequences to a fixed length\n",
    "# The vocabulary is built from the sentences, and each token is assigned a unique index.\n",
    "# The encode function converts a sentence into a sequence of indices based on the vocabulary.\n",
    "# The pad function pads the sequence to a fixed length with the <pad> token.\n",
    "# The src_vocab and tgt_vocab dictionaries map tokens to their corresponding indices.\n",
    "def encode(sentence, vocab):\n",
    "    return [vocab['<sos>']] + [vocab.get(tok, vocab['<unk>']) for tok in tokenize(sentence)] + [vocab['<eos>']]\n",
    "\n",
    "def pad(seq, max_len, pad_id):\n",
    "    return seq + [pad_id] * (max_len - len(seq))\n",
    "\n",
    "src_encoded = []\n",
    "tgt_encoded = []\n",
    "\n",
    "max_src_len = 10\n",
    "max_tgt_len = 12\n",
    "\n",
    "for src, tgt in raw_data:\n",
    "    src_seq = pad(encode(src, src_vocab), max_src_len, src_vocab['<pad>'])\n",
    "    tgt_seq = pad(encode(tgt, tgt_vocab), max_tgt_len, tgt_vocab['<pad>'])\n",
    "    src_encoded.append(src_seq)\n",
    "    tgt_encoded.append(tgt_seq)\n",
    "\n",
    "import torch\n",
    "src_batch = torch.tensor(src_encoded)\n",
    "tgt_batch = torch.tensor(tgt_encoded)\n",
    "\n",
    "# For training\n",
    "tgt_input = tgt_batch[:, :-1]\n",
    "tgt_output = tgt_batch[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "54b7528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model, Loss, and Optimizer \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Use CPU to avoid CUDA device interface issues\n",
    "device = torch.device(\"cpu\")  # Force CPU for stable execution\n",
    "\n",
    "vocab_size_src = len(src_vocab)\n",
    "vocab_size_tgt = len(tgt_vocab)\n",
    "\n",
    "# Initialize model\n",
    "model = Transformer(\n",
    "    src_vocab_size=vocab_size_src,\n",
    "    tgt_vocab_size=vocab_size_tgt,\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    d_ff=2048,\n",
    "    num_layers=2,\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab['<pad>'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Move data to device\n",
    "src_batch = src_batch.to(device)\n",
    "tgt_input = tgt_input.to(device)\n",
    "tgt_output = tgt_output.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "77e5a5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3.1701\n",
      "Epoch 2, Loss: 6.1985\n",
      "Epoch 3, Loss: 4.4693\n",
      "Epoch 4, Loss: 3.9004\n",
      "Epoch 5, Loss: 4.5901\n",
      "Epoch 6, Loss: 4.6560\n",
      "Epoch 7, Loss: 3.7179\n",
      "Epoch 8, Loss: 3.8246\n",
      "Epoch 9, Loss: 3.3376\n",
      "Epoch 10, Loss: 3.0011\n",
      "Epoch 11, Loss: 2.9379\n",
      "Epoch 12, Loss: 3.1550\n",
      "Epoch 13, Loss: 2.9331\n",
      "Epoch 14, Loss: 2.7485\n",
      "Epoch 15, Loss: 2.8254\n",
      "Epoch 16, Loss: 2.8021\n",
      "Epoch 17, Loss: 2.8220\n",
      "Epoch 18, Loss: 2.9300\n",
      "Epoch 19, Loss: 2.8761\n",
      "Epoch 20, Loss: 2.6689\n",
      "Epoch 21, Loss: 2.7213\n",
      "Epoch 22, Loss: 2.7654\n",
      "Epoch 23, Loss: 2.6871\n",
      "Epoch 24, Loss: 2.6781\n",
      "Epoch 25, Loss: 2.6909\n",
      "Epoch 26, Loss: 2.6994\n",
      "Epoch 27, Loss: 2.6649\n",
      "Epoch 28, Loss: 2.6430\n",
      "Epoch 29, Loss: 2.6289\n",
      "Epoch 30, Loss: 2.6103\n"
     ]
    }
   ],
   "source": [
    "#Training Loop \n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model(src_batch, tgt_input)  # (batch, tgt_len-1, tgt_vocab_size)\n",
    "    loss = criterion(logits.view(-1, vocab_size_tgt), tgt_output.reshape(-1))\n",
    "\n",
    "    # Backward + update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b176aad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy Decoding Function\n",
    "def greedy_decode(model, src_seq, max_len=12):\n",
    "    model.eval()\n",
    "    src = torch.tensor([src_seq], dtype=torch.long).to(device)\n",
    "    memory = model.encoder(src)\n",
    "\n",
    "    ys = torch.tensor([[tgt_vocab['<sos>']]], dtype=torch.long).to(device)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        out = model.decoder(ys, memory)\n",
    "        logits = model.output_layer(out[:, -1:])\n",
    "        next_token = torch.argmax(logits, dim=-1)[:, -1].unsqueeze(1)\n",
    "\n",
    "        ys = torch.cat([ys, next_token], dim=1)\n",
    "        if next_token.item() == tgt_vocab['<eos>']:\n",
    "            break\n",
    "    return ys.squeeze().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6c6b9198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I am a student\n",
      "Translation: \n"
     ]
    }
   ],
   "source": [
    "# Testing a translation\n",
    "def decode_tokens(token_ids, inv_vocab):\n",
    "    return [inv_vocab[i] for i in token_ids if i not in [tgt_vocab['<pad>'], tgt_vocab['<sos>'], tgt_vocab['<eos>']]]\n",
    "\n",
    "# Try translating the first sentence\n",
    "src_sentence = \"I am a student\"\n",
    "src_seq = pad(encode(src_sentence, src_vocab), max_src_len, src_vocab['<pad>'])\n",
    "\n",
    "predicted_ids = greedy_decode(model, src_seq)\n",
    "translated_tokens = decode_tokens(predicted_ids, inv_tgt_vocab)\n",
    "\n",
    "print(\"Input:\", src_sentence)\n",
    "print(\"Translation:\", \" \".join(translated_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "20892f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and Preprocessing data\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load spaCy tokenizers\n",
    "spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text.lower() for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text.lower() for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "702cff4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created 15 translation pairs\n",
      "✅ Tokenized all pairs\n",
      "✅ Vocab size: 105\n",
      "Test English: ['a', 'man', 'sits', 'on', 'a', 'bench'] → [1, 5, 22, 23, 10, 5, 24, 2]\n",
      "Test German: ['ein', 'mann', 'sitzt', 'auf', 'einer', 'bank'] → [1, 11, 25, 26, 12, 27, 28, 2]\n",
      "\n",
      "🎉 SUCCESS: Multi30k-style dataset ready without torchtext!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Build vacab and prepare subset \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load full dataset, we’ll only use a few samples\n",
    "train_iter = Multi30k(split='train')\n",
    "\n",
    "# Tokenize and extract few samples\n",
    "raw_data = []\n",
    "for i, (de, en) in enumerate(train_iter):\n",
    "    if i >= 10:  # small subset\n",
    "        break\n",
    "    raw_data.append((tokenize_en(en), tokenize_de(de)))\n",
    "\n",
    "# Build vocab\n",
    "def yield_tokens(data, tokenizer):\n",
    "    for src, tgt in data:\n",
    "        yield src\n",
    "        yield tgt\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(raw_data, None),\n",
    "                                   specials=['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "                                   special_first=True)\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "\n",
    "# Encode sequences\n",
    "def encode(tokens):\n",
    "    return [vocab['<sos>']] + [vocab[tok] for tok in tokens] + [vocab['<eos>']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c784d861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 PREPARING DATASET FOR TRAINING\n",
      "==================================================\n",
      "✅ Source batch shape: torch.Size([15, 15])\n",
      "✅ Target input shape: torch.Size([15, 14])\n",
      "✅ Target output shape: torch.Size([15, 14])\n",
      "✅ Dataset prepared with 15 samples\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch_cuda)",
   "language": "python",
   "name": "torch_cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
